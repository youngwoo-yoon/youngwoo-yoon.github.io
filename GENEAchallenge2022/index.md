---
title: The GENEA Challenge 2022
description: A large evaluation of data-driven co-speech gesture generation
---

<meta content="The GENEA Challenge 2022" property="og:title">
<meta content="A large evaluation of data-driven co-speech gesture generation" property="og:description">

### [Youngwoo Yoon\*](https://sites.google.com/view/youngwoo-yoon/), [Pieter Wolfert\*](https://www.pieterwolfert.com/), [Taras Kucherenko\*](https://svito-zar.github.io/), [Carla Viegas](https://carlaviegas.info/), [Teodor Nikolov](https://teonikolov.com/), [Mihail Tsakov](https://www.linkedin.com/in/mihailtsakov/), [Gustav Eje Henter](https://people.kth.se/~ghe/)

<br/>
<p align="center">
  <img width="600" src="video.gif">
</p>


## Summary

This webpage contains data, code, and results from the second GENEA Challenge, intended as a benchmark of data-driven automatic co-speech gesture generation. In the challenge, participating teams used a common speech and motion dataset to build gesture-generation systems. Motion generated by all these systems was then rendered to video using a standardised visualisation and evaluated in several large, crowdsourced user studies. This year's dataset was based on 18 hours of full-body motion capture, including fingers, of different persons engaging in dyadic conversation, taken from [the Talking With Hands 16.2M dataset](https://github.com/facebookresearch/TalkingWithHands32M/). Ten teams participated in the evaluation across two tiers: full-body and upper-body gesticulation. For each tier we evaluated both the human-likeness of the gesture motion and its appropriateness for the specific speech.

The evaluation results are a revolution, and a revelation: Some synthetic conditions are rated as significantly more human-like than human motion capture. At the same time, all synthetic motion is found to be vastly less appropriate for the speech than the original motion-capture recordings.

Please see our paper (link will be available soon) for more information, and the links below for the challenge data, code, and results.

## Open-source materials

* Data
  * Challenge dataset: To be uploaded
  * Annotation manual: To be uploaded
  * Submitted BVH files: To be uploaded
  * User-study video stimuli: To be uploaded
* Code
  * Visualization code: [github.com/TeoNikolov/genea_visualizer](https://github.com/TeoNikolov/genea_visualizer)
  * Objective evaluation code: [github.com/genea-workshop/genea_numerical_evaluations](https://github.com/genea-workshop/genea_numerical_evaluations)
  * Text-based baseline: [Yoon et al. (ICRA 2019)](https://github.com/youngwoo-yoon/Co-Speech_Gesture_Generation)
  * Audio-based baseline: [Kucherenko et al. (IVA 2019)](https://github.com/genea-workshop/Speech_driven_gesture_generation_with_autoencoder/tree/GENEA_2022)

* Results
  * Subjective evaluation responses, analysis, and results: [DOI 10.5281/zenodo.6940057](https://doi.org/10.5281/zenodo.6940057)
  * Objective evaluation data: To be uploaded
* Papers
  * To be uploaded
* Presentations
  * Videos to be uploaded

## Citation

If you use materials from this challenge, please cite our latest paper about the challenge. Currently, that is our paper at ICMI 2022:
```
{% raw %}@inproceedings{yoon2022genea,
  author={Yoon, Youngwoo and Wolfert, Pieter and Kucherenko, Taras and Viegas, Carla and Nikolov, Teodor and Tsakov, Mihail and Henter, Gustav Eje},
  title={{T}he {GENEA} {C}hallenge 2022: {A} large evaluation of data-driven co-speech gesture generation},
  booktitle={Proceedings of the ACM International Conference on Multimodal Interaction},
  publisher={ACM},
  series={ICMI '22},
  year={2022}
}{% endraw %}
```
Also consider citing the original paper about the motion data from Meta Research:
```
{% raw %}@inproceedings{lee2019talking,
  title={{T}alking {W}ith {H}ands 16.2{M}: {A} large-scale dataset of synchronized body-finger motion and audio for conversational motion analysis and synthesis},
  author={Lee, Gilwoo and Deng, Zhiwei and Ma, Shugao and Shiratori, Takaaki and Srinivasa, Siddhartha S. and Sheikh, Yaser},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={763--772},
  doi={10.1109/ICCV.2019.00085},
  series={ICCV '19},
  publisher={IEEE},
  year={2019}
}{% endraw %}
```

## Contact

* You can e-mail the GENEA organisers at [genea-contact@googlegroups.com](mailto:genea-contact@googlegroups.com).
* Also see [the main GENEA website](https://genea-workshop.github.io/) for more information.

[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fyoungwoo-yoon.github.io%2FGENEAchallenge2022%2F&count_bg=%233D85C8&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)
